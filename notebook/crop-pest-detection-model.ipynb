{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8098021a",
   "metadata": {},
   "source": [
    "# Crop Pest Detection Model - Kaggle Optimized\n",
    "\n",
    "This notebook trains a MobileNetV3-based pest classifier using transfer learning on Kaggle.\n",
    "\n",
    "**Dataset:** [Pest Dataset](https://www.kaggle.com/datasets/simranvolunesia/pest-dataset)\n",
    "\n",
    "**Instructions:**\n",
    "1. Add the dataset to your Kaggle notebook: Click \"+ Add Data\" → Search \"pest-dataset\" → Add\n",
    "2. Enable GPU: Settings → Accelerator → GPU T4 x2\n",
    "3. Run all cells sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ac639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q tensorflow numpy pandas matplotlib seaborn scikit-learn pillow\n",
    "\n",
    "print(\"✓ Packages installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20205a",
   "metadata": {},
   "source": [
    "## 2. Setup Directories\n",
    "\n",
    "Configure paths for Kaggle environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8257ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Kaggle dataset path\n",
    "KAGGLE_INPUT = '/kaggle/input/pest-dataset/pest'\n",
    "\n",
    "# Working directory paths\n",
    "WORKING_DIR = '/kaggle/working'\n",
    "MODELS_DIR = os.path.join(WORKING_DIR, 'models')\n",
    "\n",
    "# Dataset paths (using Kaggle input)\n",
    "TRAIN_DIR = os.path.join(KAGGLE_INPUT, 'train')\n",
    "TEST_DIR = os.path.join(KAGGLE_INPUT, 'test')\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Verify dataset structure\n",
    "print(\"=\"*60)\n",
    "print(\"KAGGLE ENVIRONMENT SETUP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Kaggle Input: {KAGGLE_INPUT}\")\n",
    "print(f\"Train Directory: {TRAIN_DIR}\")\n",
    "print(f\"Test Directory: {TEST_DIR}\")\n",
    "print(f\"Models Directory: {MODELS_DIR}\")\n",
    "print()\n",
    "\n",
    "# Check if dataset exists\n",
    "if os.path.exists(TRAIN_DIR) and os.path.exists(TEST_DIR):\n",
    "    print(\"✓ Dataset found successfully!\")\n",
    "    print(f\"  Train classes: {len(os.listdir(TRAIN_DIR))}\")\n",
    "    print(f\"  Test classes: {len(os.listdir(TEST_DIR))}\")\n",
    "else:\n",
    "    print(\"⚠ Dataset not found!\")\n",
    "    print(\"\\nPlease add the dataset:\")\n",
    "    print(\"  1. Click '+ Add Data' button\")\n",
    "    print(\"  2. Search for 'pest-dataset' by simranvolunesia\")\n",
    "    print(\"  3. Click 'Add' to attach it to this notebook\")\n",
    "    raise FileNotFoundError(\"Dataset not found. Please add the pest-dataset to your Kaggle notebook.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a3f678",
   "metadata": {},
   "source": [
    "## 3. Data Exploration\n",
    "\n",
    "Explore the dataset structure and visualize sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a29efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Count images per class\n",
    "def count_images_in_dir(directory):\n",
    "    counts = {}\n",
    "    for class_name in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            image_files = [f for f in os.listdir(class_path) \n",
    "                          if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "            counts[class_name] = len(image_files)\n",
    "    return counts\n",
    "\n",
    "train_counts = count_images_in_dir(TRAIN_DIR)\n",
    "test_counts = count_images_in_dir(TEST_DIR)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal classes: {len(train_counts)}\")\n",
    "print(f\"Total training images: {sum(train_counts.values())}\")\n",
    "print(f\"Total test images: {sum(test_counts.values())}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "for class_name in sorted(train_counts.keys()):\n",
    "    print(f\"  {class_name:20s}: {train_counts[class_name]:4d} train, {test_counts.get(class_name, 0):4d} test\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcead61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Create dataframe\n",
    "df_counts = pd.DataFrame({\n",
    "    'Class': list(train_counts.keys()),\n",
    "    'Train': list(train_counts.values()),\n",
    "    'Test': [test_counts.get(c, 0) for c in train_counts.keys()]\n",
    "})\n",
    "\n",
    "# Plot\n",
    "df_counts_sorted = df_counts.sort_values('Train', ascending=False)\n",
    "x = np.arange(len(df_counts_sorted))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, df_counts_sorted['Train'], width, label='Train', alpha=0.8)\n",
    "plt.bar(x + width/2, df_counts_sorted['Test'], width, label='Test', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Pest Class', fontsize=12)\n",
    "plt.ylabel('Number of Images', fontsize=12)\n",
    "plt.title('Dataset Distribution by Class', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x, df_counts_sorted['Class'], rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from each class\n",
    "class_names = sorted(train_counts.keys())\n",
    "num_classes = len(class_names)\n",
    "\n",
    "fig, axes = plt.subplots(3, min(5, num_classes), figsize=(15, 9))\n",
    "fig.suptitle('Sample Images from Each Class', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, class_name in enumerate(class_names[:15]):  # Show max 15 classes\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    \n",
    "    class_dir = os.path.join(TRAIN_DIR, class_name)\n",
    "    images = [f for f in os.listdir(class_dir) \n",
    "             if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "    \n",
    "    if images:\n",
    "        img_path = os.path.join(class_dir, images[0])\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        if num_classes <= 5:\n",
    "            axes[col].imshow(img)\n",
    "            axes[col].set_title(class_name, fontsize=10)\n",
    "            axes[col].axis('off')\n",
    "        else:\n",
    "            axes[row, col].imshow(img)\n",
    "            axes[row, col].set_title(class_name, fontsize=10)\n",
    "            axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131fbc5",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Set up data generators with augmentation for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1262a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataPreprocessor for Kaggle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, img_size=(224, 224), batch_size=32):\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def create_train_generator(self):\n",
    "        return ImageDataGenerator(\n",
    "            preprocessing_function=preprocess_input,\n",
    "            rotation_range=30,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            fill_mode='nearest',\n",
    "            validation_split=0.2\n",
    "        )\n",
    "    \n",
    "    def create_test_generator(self):\n",
    "        return ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    \n",
    "    def load_train_data(self, train_dir):\n",
    "        train_datagen = self.create_train_generator()\n",
    "        train_gen = train_datagen.flow_from_directory(\n",
    "            train_dir, target_size=self.img_size, batch_size=self.batch_size,\n",
    "            class_mode='categorical', subset='training', shuffle=True\n",
    "        )\n",
    "        val_gen = train_datagen.flow_from_directory(\n",
    "            train_dir, target_size=self.img_size, batch_size=self.batch_size,\n",
    "            class_mode='categorical', subset='validation', shuffle=True\n",
    "        )\n",
    "        return train_gen, val_gen\n",
    "    \n",
    "    def load_test_data(self, test_dir):\n",
    "        test_datagen = self.create_test_generator()\n",
    "        return test_datagen.flow_from_directory(\n",
    "            test_dir, target_size=self.img_size, batch_size=self.batch_size,\n",
    "            class_mode='categorical', shuffle=False\n",
    "        )\n",
    "\n",
    "print(\"✓ DataPreprocessor ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c3a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "preprocessor = DataPreprocessor(img_size=IMG_SIZE, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading training and validation data...\")\n",
    "train_generator, validation_generator = preprocessor.load_train_data(TRAIN_DIR)\n",
    "\n",
    "print(\"\\nLoading test data...\")\n",
    "test_generator = preprocessor.load_test_data(TEST_DIR)\n",
    "\n",
    "# Get class information\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"\\n✓ Data loaded successfully!\")\n",
    "print(f\"  Classes: {num_classes}\")\n",
    "print(f\"  Training samples: {train_generator.samples}\")\n",
    "print(f\"  Validation samples: {validation_generator.samples}\")\n",
    "print(f\"  Test samples: {test_generator.samples}\")\n",
    "print(f\"\\n  Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1043ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmented images\n",
    "print(\"Visualizing data augmentation...\")\n",
    "\n",
    "# Get a batch of images\n",
    "sample_batch = next(train_generator)\n",
    "sample_images = sample_batch[0][:9]\n",
    "sample_labels = sample_batch[1][:9]\n",
    "\n",
    "# Denormalize images for display\n",
    "def denormalize_image(img):\n",
    "    img = img.copy()\n",
    "    img += 1\n",
    "    img *= 127.5\n",
    "    return np.clip(img, 0, 255).astype(np.uint8)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "fig.suptitle('Augmented Training Images', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(sample_images):\n",
    "        img = denormalize_image(sample_images[i])\n",
    "        label_idx = np.argmax(sample_labels[i])\n",
    "        label = class_names[label_idx]\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        ax.set_title(label, fontsize=11)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6b5c4",
   "metadata": {},
   "source": [
    "## 5. Model Development\n",
    "\n",
    "Build MobileNetV3-based classifier with transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Build model\n",
    "def build_mobilenet_classifier(num_classes, img_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Build MobileNetV3-based classifier.\n",
    "    \"\"\"\n",
    "    # Load MobileNetV3 with ImageNet weights\n",
    "    base_model = MobileNetV3Large(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(*img_size, 3),\n",
    "        include_preprocessing=False\n",
    "    )\n",
    "    \n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build model\n",
    "    inputs = keras.Input(shape=(*img_size, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs, name='crop_pest_detector')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "print(f\"\\nBuilding model for {num_classes} classes...\")\n",
    "model = build_mobilenet_classifier(num_classes, IMG_SIZE)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "model.summary()\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b014b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy'),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"✓ Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c9319",
   "metadata": {},
   "source": [
    "## 6. Model Training\n",
    "\n",
    "Train the model with callbacks for early stopping and model checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad8da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS = 50\n",
    "MODEL_SAVE_PATH = os.path.join(MODELS_DIR, 'crop_pest_model.h5')\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        MODEL_SAVE_PATH,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4565991",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "Evaluate the trained model and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 0].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "    axes[0, 0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "    axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 1].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "    axes[0, 1].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "    axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # Precision\n",
    "    axes[1, 0].plot(history.history['precision'], label='Train', linewidth=2)\n",
    "    axes[1, 0].plot(history.history['val_precision'], label='Validation', linewidth=2)\n",
    "    axes[1, 0].set_title('Model Precision', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Recall\n",
    "    axes[1, 1].plot(history.history['recall'], label='Train', linewidth=2)\n",
    "    axes[1, 1].plot(history.history['val_recall'], label='Validation', linewidth=2)\n",
    "    axes[1, 1].set_title('Model Recall', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Recall')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a85e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_loss, test_acc, test_top3, test_precision, test_recall = model.evaluate(test_generator)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Accuracy:       {test_acc*100:.2f}%\")\n",
    "print(f\"Test Top-3 Accuracy: {test_top3*100:.2f}%\")\n",
    "print(f\"Test Precision:      {test_precision*100:.2f}%\")\n",
    "print(f\"Test Recall:         {test_recall*100:.2f}%\")\n",
    "print(f\"Test Loss:           {test_loss:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb6d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\nGenerating predictions for test set...\")\n",
    "test_generator.reset()\n",
    "predictions = model.predict(test_generator, verbose=1)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a2f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class performance\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n",
    "\n",
    "# Create dataframe\n",
    "performance_df = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-CLASS PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "print(performance_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize per-class metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    performance_df_sorted = performance_df.sort_values(metric, ascending=True)\n",
    "    axes[idx].barh(performance_df_sorted['Class'], performance_df_sorted[metric], color=color, alpha=0.7)\n",
    "    axes[idx].set_xlabel(metric, fontsize=12)\n",
    "    axes[idx].set_title(f'{metric} by Class', fontsize=14, fontweight='bold')\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "    axes[idx].set_xlim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592da097",
   "metadata": {},
   "source": [
    "## 8. Fine-tuning\n",
    "\n",
    "Fine-tune the model by unfreezing some base layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d58100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning configuration\n",
    "FINE_TUNE_EPOCHS = 20\n",
    "FINE_TUNE_LR = 1e-5\n",
    "UNFREEZE_LAYERS = 50\n",
    "FINE_TUNED_MODEL_PATH = os.path.join(MODELS_DIR, 'crop_pest_model_finetuned.h5')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINE-TUNING MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Unfreezing last {UNFREEZE_LAYERS} layers of base model\")\n",
    "print(f\"Fine-tuning learning rate: {FINE_TUNE_LR}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Unfreeze base model layers\n",
    "base_model = model.layers[1]  # MobileNetV3 is the second layer\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all except last UNFREEZE_LAYERS\n",
    "for layer in base_model.layers[:-UNFREEZE_LAYERS]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=FINE_TUNE_LR),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy'),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Callbacks for fine-tuning\n",
    "fine_tune_callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        FINE_TUNED_MODEL_PATH,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=7,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Fine-tune\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=FINE_TUNE_EPOCHS,\n",
    "    callbacks=fine_tune_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINE-TUNING COMPLETED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0354f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fine-tuned model\n",
    "print(\"\\nEvaluating fine-tuned model on test set...\")\n",
    "test_loss_ft, test_acc_ft, test_top3_ft, test_precision_ft, test_recall_ft = model.evaluate(test_generator)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINE-TUNED MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Accuracy:       {test_acc_ft*100:.2f}% (was {test_acc*100:.2f}%)\")\n",
    "print(f\"Test Top-3 Accuracy: {test_top3_ft*100:.2f}% (was {test_top3*100:.2f}%)\")\n",
    "print(f\"Test Precision:      {test_precision_ft*100:.2f}% (was {test_precision*100:.2f}%)\")\n",
    "print(f\"Test Recall:         {test_recall_ft*100:.2f}% (was {test_recall*100:.2f}%)\")\n",
    "print(f\"Test Loss:           {test_loss_ft:.4f} (was {test_loss:.4f})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Plot fine-tuning history\n",
    "plot_training_history(history_fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7327f3",
   "metadata": {},
   "source": [
    "## 9. Save Model and Export\n",
    "\n",
    "Save the final model for download and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc7ed30",
   "metadata": {},
   "source": [
    "## 10. Sample Prediction\n",
    "\n",
    "Test the model with a sample prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Save class names\n",
    "class_names_path = os.path.join(MODELS_DIR, 'class_names.json')\n",
    "with open(class_names_path, 'w') as f:\n",
    "    json.dump(class_names, f, indent=2)\n",
    "print(f\"✓ Class names saved to: {class_names_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"model_path\": FINE_TUNED_MODEL_PATH,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"class_names\": class_names,\n",
    "    \"image_size\": list(IMG_SIZE),\n",
    "    \"architecture\": \"MobileNetV3Large\",\n",
    "    \"framework\": \"TensorFlow/Keras\",\n",
    "    \"training\": {\n",
    "        \"initial_epochs\": EPOCHS,\n",
    "        \"fine_tune_epochs\": FINE_TUNE_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"initial_lr\": LEARNING_RATE,\n",
    "        \"fine_tune_lr\": FINE_TUNE_LR\n",
    "    },\n",
    "    \"performance\": {\n",
    "        \"test_accuracy\": float(test_acc_ft),\n",
    "        \"test_top3_accuracy\": float(test_top3_ft),\n",
    "        \"test_precision\": float(test_precision_ft),\n",
    "        \"test_recall\": float(test_recall_ft),\n",
    "        \"test_loss\": float(test_loss_ft)\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(MODELS_DIR, 'model_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"✓ Model metadata saved to: {metadata_path}\")\n",
    "\n",
    "# Save model summary\n",
    "summary_path = os.path.join(MODELS_DIR, 'model_summary.txt')\n",
    "with open(summary_path, 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "print(f\"✓ Model summary saved to: {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL FILES SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel files:\")\n",
    "print(f\"  - Initial model: {MODEL_SAVE_PATH}\")\n",
    "print(f\"  - Fine-tuned model: {FINE_TUNED_MODEL_PATH}\")\n",
    "print(f\"  - Class names: {class_names_path}\")\n",
    "print(f\"  - Metadata: {metadata_path}\")\n",
    "print(f\"  - Model summary: {summary_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15600a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prediction on a test image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
    "\n",
    "def predict_image(model, image_path, class_names, top_k=3):\n",
    "    \"\"\"\n",
    "    Predict pest class for a single image.\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    img = load_img(image_path, target_size=IMG_SIZE)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(img_array, verbose=0)\n",
    "    \n",
    "    # Get top K predictions\n",
    "    top_indices = np.argsort(predictions[0])[-top_k:][::-1]\n",
    "    top_probs = predictions[0][top_indices]\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Predicted: {class_names[top_indices[0]]}\\nConfidence: {top_probs[0]*100:.2f}%\",\n",
    "             fontsize=12, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    colors = ['#2ecc71' if i == 0 else '#3498db' for i in range(top_k)]\n",
    "    plt.barh([class_names[i] for i in top_indices], top_probs, color=colors, alpha=0.7)\n",
    "    plt.xlabel('Confidence', fontsize=11)\n",
    "    plt.title('Top Predictions', fontsize=12, fontweight='bold')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nPrediction Results:\")\n",
    "    for i, (idx, prob) in enumerate(zip(top_indices, top_probs), 1):\n",
    "        print(f\"  {i}. {class_names[idx]:20s} - {prob*100:6.2f}%\")\n",
    "\n",
    "# Get a random test image\n",
    "import random\n",
    "test_class = random.choice(class_names)\n",
    "test_class_dir = os.path.join(TEST_DIR, test_class)\n",
    "test_images = [f for f in os.listdir(test_class_dir) \n",
    "              if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "\n",
    "if test_images:\n",
    "    sample_image = os.path.join(test_class_dir, random.choice(test_images))\n",
    "    print(f\"\\nTesting with image from class: {test_class}\")\n",
    "    print(f\"Image path: {sample_image}\\n\")\n",
    "    predict_image(model, sample_image, class_names)\n",
    "else:\n",
    "    print(\"No test images available for prediction demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1609e5c0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully:\n",
    "\n",
    "1. ✓ Set up Kaggle environment and verified dataset\n",
    "2. ✓ Explored and visualized the Pest Dataset\n",
    "3. ✓ Implemented data preprocessing with augmentation\n",
    "4. ✓ Built a MobileNetV3-based classifier using transfer learning\n",
    "5. ✓ Trained the model with early stopping and checkpointing\n",
    "6. ✓ Evaluated the model with comprehensive metrics\n",
    "7. ✓ Fine-tuned the model for improved performance\n",
    "8. ✓ Saved the model with metadata for deployment\n",
    "\n",
    "**Download Your Model:**\n",
    "All files are saved in `/kaggle/working/models/`:\n",
    "- `crop_pest_model_finetuned.h5` - Fine-tuned model\n",
    "- `class_names.json` - Class labels\n",
    "- `model_metadata.json` - Model information\n",
    "\n",
    "To download: Click on the **Output** tab → Download the models folder"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
